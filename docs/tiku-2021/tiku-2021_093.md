# 二千零二十一、届阅文机器学习方向笔试卷

## 1

为什么要进行归一化，有哪些归一化方法，那些模型需要归一化，那些不需要，为什么

你的答案

本题知识点

算法工程师 阅文集团 2021

讨论

[零葬](https://www.nowcoder.com/profile/75718849)

需要进行归一化的原因：模型中存在对不同特征进行线性加权求和这种操作的时候通常都需要对特征进行归一化操作（如 LR，SVM，以及深度学习的模型）。这样的广义线性模型一般在训练的过程中都涉及到梯度下降的过程，而特征之间的取值范围相差悬殊时往往会对这样的数值求解方法产生不良影响（如梯度消失、梯度爆炸），导致模型不收敛。因此常通过特征归一化来消除特征量纲对模型训练的影响。常用的归一化方法：最大最小值归一化、去均值/标准差、去中值/标准差，深度学习中还有 Batch Normalization、Layer Normalization 等归一化方法。不需要进行归一化的模型：决策树、GBDT、XGBoost、LightGBM、CatBoost 这样的树模型，它们在训练的过程中通过信息增益、信息增益率、gini 系数来进行树的生长，不涉及梯度下降过程。

编辑于 2021-01-16 13:06:28

* * *

[食堂在逃干饭王](https://www.nowcoder.com/profile/5786634)

数据中各特征的值域不同，在使用梯度下降算法更新模型参数时，如果不对特征进行归一化处理，会影响参数更新速率，使模型需要更长的时间才能收敛。还有一些模型是基于距离设计的，量纲对模型影响较大，因此需要对特征进行归一化。常见的归一化方法有：最小最大归一化、z-score（零均值）归一化使用梯度下降更新模型参数的模型需要对数据归一化，如 LR、DNN 等；基于距离设计而需要统一量纲的模型也需要对数据归一化，如 KNN、SVM、K-means 等。而不使用梯度下降的则不需要，如决策树-base 的模型，像 DT、RF、GBDT、AdaBoost、XGBoost、LGBM 等。

编辑于 2021-02-04 12:39:12

* * *

[水上漂](https://www.nowcoder.com/profile/8752997)

树模型不需要归一化

发表于 2021-01-17 23:33:42

* * *

## 2

 Batch Normalization 层原理，作用

你的答案

本题知识点

算法工程师 阅文集团 2021

讨论

[零葬](https://www.nowcoder.com/profile/75718849)

1\. BN 概念

传统的神经网络，只是在将样本 x 输入输入层之前对 x 进行标准化处理，以降低样本间的差异性。BN 是在此基础上，不仅仅只对输入层的输入数据 x 进行标准化，还对每个隐藏层的输入进行标准化。2\. Covariate Shift 问题 Convariate shift 是 BN 论文作者提出来的概念，指的是具有不同分布的输入值对深度网络学习的影响。当神经网络的输入值的分布不同时，我们可以理解为输入特征值的 scale 差异较大，与权重进行矩阵相乘后，会产生一些偏离较大地差异值；而深度学习网络需要通过训练不断更新完善，那么差异值产生的些许变化都会深深影响后层，偏离越大表现越为明显；因此，对于反向传播来说，这些现象都会导致梯度发散，从而需要更多的训练步骤来抵消 scale 不同带来的影响，也就是说，这种分布不一致将减缓训练速度。而 BN 的作用就是将这些输入值进行标准化，降低 scale 的差异至同一个范围内。这样做的好处在于一方面提高梯度的收敛程度，加快模型的训练速度；另一方面使得每一层可以尽量面对同一特征分布的输入值，减少了变化带来的不确定性，也降低了对后层网路的影响，各层网路变得相对独立，缓解了训练中的梯度消失问题。3\. BN 的作用(1) 缓解 DNN 训练中的梯度消失问题(2) 加快模型的训练速度

发表于 2021-01-15 10:32:37

* * *

[水上漂](https://www.nowcoder.com/profile/8752997)

极快速度，剃度消失

发表于 2021-01-17 23:35:12

* * *

## 3

比较 LR 和 GBDT，什么情景下 GBDT 不如 LR

你的答案

本题知识点

算法工程师 阅文集团 2021

讨论

[零葬](https://www.nowcoder.com/profile/75718849)

1.比较 LR 和 GBDT：(1) LR 是一种线性模型，而 GBDT 是一种非线性的树模型，因此通常为了增强模型的非线性表达能力，使用 LR 模型之前会有非常繁重的特征工程任务；(2) LR 是单模，而 GBDT 是集成模型，通常来说，在数据低噪的情况下，GBDT 的效果都会优于 LR；(3) LR 采用梯度下降方法进行训练，需要对特征进行归一化操作，而 GBDT 在训练的过程中基于 gini 系数选择特征，计算最优的特征值切分点，可以不用做特征归一化。2.GBDT 不如 LR 的地方：一方面，当需要对模型进行解释的时候，GBDT 显然会比 LR 更加“黑盒”，因为我们不可能去解释每一棵树。相比之下，LR 的特征权重能够很直观地反映出特征对不同类样本的贡献程度，也正因为如此好理解，很多时候我们可以根据 LR 模型得到的分析结论做出更有说服力的营销和运营策略；另一方面，LR 模型的大规模并行训练已经非常成熟，模型迭代速度很快，业务人员可以很快得到模型的反馈，并对模型进行针对性的修正。而 GBDT 这样的串行集成方式让它的并行十分困难，在大数据规模下训练速度十分缓慢；最后，对于高维的稀疏数据，GBDT 往往很容易过拟合，将这些无用信息学习到模型，得到很深的树；而 LR 这样的线性模型可以通过加入正则化，来对特征进行筛选，降低弱特征的权重（L2 正则）甚至过滤掉弱特征（L1 正则），从而削弱模型的复杂度，防止过拟合。

编辑于 2021-02-25 11:30:43

* * *

[食堂在逃干饭王](https://www.nowcoder.com/profile/5786634)

LR 是线性模型，模型简单，可解释性强，对异常特征不敏感 GBDT 是非线性模型，属于集成学习中的 boosting 方法，基学习器是树模型，且树模型间相互依赖，无法并行训练，其特征组合和表达能力更强，且更容易过拟合高维稀疏特征时，LR 会比 GBDT 更好，因为 LR 加正则化不容易过拟合，而 GBDT 在高维稀疏特征下会生成很深的树，容易过拟合。

发表于 2021-02-04 12:17:58

* * *

[水上漂](https://www.nowcoder.com/profile/8752997)

模样

发表于 2021-01-17 23:38:24

* * *

## 4

常见的决策树算法有哪些？请描述它们在进行树的生成过程中，具体的特征选择算法，以及它们的对比？

你的答案

本题知识点

算法工程师 阅文集团 2021

讨论

[零葬](https://www.nowcoder.com/profile/75718849)

ID3：通过信息增益来选择特征，但是对取值较多的特征有一定的偏好；C4.5：通过信息增益率来选择特征，对取值较多特征的偏好进行了压制；CART：通过 gini 系数来选择特征，与以上两种仅用于分类的算法相比，CART 树可用于分类和回归两种任务。

发表于 2021-01-15 10:17:10

* * *

[食堂在逃干饭王](https://www.nowcoder.com/profile/5786634)

ID3，信息增益，对取值多的特征有偏好；C4.5，信息增益比，对取值少的特征有偏好；CART，基尼指数，代表信息的不纯度，Gini 指数越小数据越纯，在保留熵的前提下简化了计算，支持分类与回归，生成的树是二叉树。

发表于 2021-02-04 12:20:38

* * *

[水上漂](https://www.nowcoder.com/profile/8752997)

如解答

发表于 2021-01-17 23:39:24

* * *

## 5

深度学习中，常用的层有哪些，各有什么作用和特点？

你的答案

本题知识点

算法工程师 阅文集团 2021

讨论

[零葬](https://www.nowcoder.com/profile/75718849)

1.  全连接层：最基础的神经网络层，每个节点接收上一层的全部节点作为输入，计算代价较大。

2.  卷积层:由多个卷积核生成而来，采用了局部视野、权值共享的思想。

3.  池化层:缩减输入数据规模，增强鲁棒性

4.  激活层:传统是 softmax，带来梯度消失于梯度爆炸问题，增加 ReLU 激活函数后，更多层次的深度学习才可进行。

5.  RNN 层：考虑时间、顺序特征，每个神经元的当前状态也被其上一个时间的状态所影响。

6.  LSTM 层：在 RNN 的基础上增加输入门、遗忘门、输出门的概念、可以支持更远距离的信息保留。

7.  GRU 层：在 LSTM 层上作了约束，只有更新门与重置门，也因为参数更少因此更容易收敛，但是数据集很大的情况下，LSTM 表达性能更好

8.  Dropout 层：是指在深度学习网络的训练过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃

9.  Batch Normalization 层：批规范化，在每次 SGD 时，通过 mini-batch 来对相应的 activation 做规范化操作，使得结果（输出信号各个维度）的均值为 0，方差为 1\. 而最后的“scale and shift”操作则是为了让因训练所需而“刻意”加入的 BN 能够有可能还原最初的输入解决了反向传播过程中的梯度问题（梯度消失和爆炸），同时使得不同 scale 的 整体更新步调更一致。

编辑于 2021-01-15 11:08:42

* * *

[水上漂](https://www.nowcoder.com/profile/8752997)

如解答

发表于 2021-01-17 23:41:14

* * *

## 6

机器学习中常见的最优化方法有哪些，请简述它们的原理、缺陷以及改进。

你的答案

本题知识点

算法工程师 阅文集团 2021

讨论

[牛客一宇](https://www.nowcoder.com/profile/958519006)

<p>梯度下降算法</p>计算 loss 的梯度，使参数向梯度负方向移动，直到最优。<p>牛顿法</p>考虑了 loss 的二阶导，即梯度的变化趋势，可以更全面的确定搜索方向，具有二阶的收敛速度。<p>随机梯度下降算法</p>随机选取一个样本来计算梯度，计算速度快，但可能不收敛，因为一个样本的梯度可能并不是真正的梯度。<p>mini batch 梯度下降算法</p>选取小批量样本计算梯度，即考虑了计算速度也考虑了收敛问题。<p>动量梯度下降</p>在梯度更新项加一个原来梯度的动量项，使其保持一定的移动惯性，从而冲出局部极小点，还可克服线路震荡，加快收敛。<p>NAG</p>在动量梯度下降基础上，其梯度计算，从原 x 点处改为从 x 点按惯性前进一步处计算，在每一步都多走一小步，有时可加快收敛。<p>步长优化</p>随着迭代次数增加使步长越来越小，可以防止其在极值点处震荡。Adam：结合了步长优化和 NAG

发表于 2021-06-14 22:54:52

* * *

[牛客 6107965 号](https://www.nowcoder.com/profile/6107965)

梯度下降算法

牛顿法

随机梯度下降算法

mini batch 梯度下降算法

动量梯度下降

RMSprop

Adam

发表于 2021-01-20 21:52:27

* * *

## 7

常见的聚类算法有哪些，请描述它们的原理。

你的答案

本题知识点

算法工程师 阅文集团 2021

讨论

[零葬](https://www.nowcoder.com/profile/75718849)

1.基于划分

给定一个有 N 个元组或者纪录的数据集，分裂法将构造 K 个分组，每一个分组就代表一个聚类，K<N。
特点： 计算量大。很适合发现中小规模的数据库中小规模的数据库中的球状簇。
算法： K-MEANS 算法、K-MEDOIDS 算法、CLARANS 算法

2.基于层次

对给定的数据集进行层次似的分解，直到某种条件满足为止。具体又可分为“自底向上”和“自顶向下”两种方案。
特点： 较小的计算开销。然而这种技术不能更正错误的决定。
算法： BIRCH 算法、CURE 算法、CHAMELEON 算法

3.基于密度

只要一个区域中的点的密度大过某个阈值，就把它加到与之相近的聚类中去。
特点： 能克服基于距离的算法只能发现“类圆形”的聚类的缺点。
算法： DBSCAN 算法、OPTICS 算法、DENCLUE 算法

4.基于网格

将数据空间划分成为有限个单元（cell）的网格结构,所有的处理都是以单个的单元为对象的。
特点： 处理速度很快，通常这是与目标数据库中记录的个数无关的，只与把数据空间分为多少个单元有关。
算法： STING 算法、CLIQUE 算法、WAVE-CLUSTER 算法

发表于 2021-01-15 11:02:39

* * *

## 8

请简介 word2vec 的原理，以及在实现过程中会涉及到的参数与参数解释。

你的答案

本题知识点

算法工程师 阅文集团 2021

讨论

[牛客 6107965 号](https://www.nowcoder.com/profile/6107965)

将原来 onehot 编码转化成低密度，有相似性的词向量，

具体实现需要负采样，霍夫曼树编码，

发表于 2021-01-20 22:00:37

* * *

## 9

请简述生成式模型与判别式模型的区别，并简单列举几个分别属于生成式模型与判别式模型的常见算法

你的答案

本题知识点

算法工程师 阅文集团 2021

讨论

[零葬](https://www.nowcoder.com/profile/75718849)

生成式模型(Generative Model)与判别式模型(Discrimitive Model)它们的区别在于：假设有样本输入值（或者观察值）x，类别标签（或者输出值）y 判别式模型评估对象是最大化条件概率 p(y|x)并直接对其建模，生成式模型评估对象是最大化联合概率 p(x,y)并对其建模。其实两者的评估目标都是要得到最终的类别标签 Y， 而 Y=argmax p(y|x)，不同的是判别式模型直接通过解在满足训练样本分布下的最优化问题得到模型参数，主要用到拉格朗日乘算法、梯度下降法，常见的判别式模型如最大熵模型、CRF、LR、SVM 等；而生成式模型先经过贝叶斯转换成 Y = argmax p(y|x) = argmax p(x|y)*p(y)，然后分别学习 p(y)和 p(x|y)的概率分布，主要通过极大似然估计的方法学习参数，常见的判别式模型如 NGram、HMM、Naive Bayes。

编辑于 2021-01-15 11:07:07

* * *

## 10

机器学习中几乎都可以看到损失函数后面会添加一个额外项，一般选用 L1 正则化和 L2 正则化，请简述他们的原理与之间的区别。

你的答案

本题知识点

算法工程师 阅文集团 2021

讨论

[零葬](https://www.nowcoder.com/profile/75718849)

L1 会趋向于产生少量的特征，而其他的特征都是 0，而 L2 会选择更多的特征，但这些特征都会接近于 0。在所有特征中只有少数特征起重要作用的情况下，选择 L1 比较合适，因为它能自动选择特征。而如果所有特征中，大部分特征都能起作用，而且起的作用很平均，那么使用 L2 也许更合适。L1 不仅可以作为正则化手段，其在特征选择时候非常有用，而 L2 就只是一种规则化而已。

编辑于 2021-01-15 11:06:21

* * *

[牛客 874133995 号](https://www.nowcoder.com/profile/874133995)

L1 会趋向于产生少量的特征，而其他的特征都是 0，而 L2 会选择更多的特征，但这些特征都会接近于 0。在所有特征中只有少数特征起重要作用的情况下，选择 L1 比较合适，因为它能自动选择特征。而如果所有特征中，大部分特征都能起作用，而且起的作用很平均，那么使用 L2 也许更合适。L1 不仅可以作为正则化手段，其在特征选择时候非常有用，而 L2 就只是一种规则化而已

编辑于 2022-02-28 17:10:44

* * *

[牛客 647419473 号](https://www.nowcoder.com/profile/647419473)

正则化的不同其实就是范数的不同，L1 正则化对所有参数的·惩罚力度都是一样，减少的是一个常量，可以让一部分权重变为 0，因此产生稀疏模型能够去除某些特征，减少储存空间。L2 正则化减少了权重的固定比例，使得权重平滑，L2 正则化不会使得权重变为 0，不会产生稀疏模型，可以选择更多的特，实现简单，能够起到正则化的作用。

发表于 2022-02-28 10:26:14

* * *

## 11

逻辑回归，相比于线性回归，有何异同？请简述逻辑回归的原理

你的答案

本题知识点

算法工程师 阅文集团 2021

讨论

[零葬](https://www.nowcoder.com/profile/75718849)

1.联系：都是线性模型，线性回归+sigmoid 函数=逻辑回归 2.区别：(1) 功能不同：线性回归是做回归的，逻辑回归是做分类的。(2) 参数求解方法不同：线性回归是用最小二乘法求解参数，逻辑回归是用梯度上升法求解参数。

发表于 2021-01-15 10:58:42

* * *

## 12

有哪些评估机器学习模型效果的指标？请简述他们的计算方法。

你的答案

本题知识点

算法工程师 阅文集团 2021

讨论

[零葬](https://www.nowcoder.com/profile/75718849)

分类 precision：TP/(TP+FP)recall：TP/(TP+FN)f1-score：2*precision*recall/(precision + recall)accuracy：分类正确的样本数/总样本数回归 mae：1/n*sum(abs(y_i-yhat_i))mse：1/n*sum((y_i-yhat_i)²)

发表于 2021-01-15 10:56:23

* * *

## 13

这里有一堆物资待分配，物资总数量不超过 200（![](img/3ab83e3574d2a70e9ebf0c46db5e9a93.svg)），每件物资重量不超过 100（![](img/c1f28ac06329a9d0b41324e72159d6a9.svg)）。请问是否可以将这堆物资分配给两个队伍，使得两个队伍的全部的物资重量和相等。

本题知识点

算法工程师 阅文集团 2021

讨论

[零葬](https://www.nowcoder.com/profile/75718849)

其实就是 0-1 背包问题

```cpp
class Solution:
    def canPartition(self , nums):
        # write code here
        if nums == []:
            return True
        if sum(nums) % 2 == 1:
            return False
        target = int(sum(nums) / 2)
        dp = [True] + [False] * target
        for i, num in enumerate(nums):
            for j in range(target, num - 1, -1):
                dp[j] |= dp[j - num]
        return dp[target]
```

发表于 2021-01-15 11:05:02

* * *

[牛客 710082035 号](https://www.nowcoder.com/profile/710082035)

就是一个 0-1 背包的变形，我们首先考虑能不能对半分，不能直接返回，如果能，那么我们就类似的把背包的空间看成 sum/2，每一个物品的重量既是重量也是需要的空间，我们最后只需要看能不能装满 sum/2 即可。

```cpp
class Solution {
public:
    /**
     * 
     * @param nums int 整型一维数组 
     * @param numsLen int nums 数组长度
     * @return bool 布尔型
     */

    int dp[10000000];
    bool canPartition(int* nums, int numsLen) {
       int sum=0;
        for(int i=0;i<numsLen;i++)
            sum+=nums[i];
        if(sum%2!=0)
            return false;
        else
        {
            sum/=2;
            for(int i=0;i<numsLen;i++)
                for(int j=sum;j>=nums[i];j--)
                {
                    //if(j-nums[i]>=0)
                    dp[j]=max(dp[j],dp[j-nums[i]]+nums[i]);
                }
            if(sum==dp[sum])
                return true;
            else
                return false;
        }
        //return true;
    }
};
```

发表于 2021-03-12 21:59:23

* * *

[生活原本沉闷，跑起来就有风](https://www.nowcoder.com/profile/583816166)

class Solution:
    def canPartition(self , nums):
        # write code here
        if nums == []:
            return True
        if sum(nums) % 2 == 1:
            return False
        target = int(sum(nums) / 2)
        dp = [True] + [False] * target
        for i, num in enumerate(nums):
            for j in range(target, num - 1, -1):
                dp[j] |= dp[j - num]
        return dp[target]

发表于 2021-01-15 16:10:47

* * *