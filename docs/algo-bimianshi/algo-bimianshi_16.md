# 第五章 第 2 节 归一化、特征稀疏处理及面试题总结

> 原文：[`www.nowcoder.com/tutorial/10067/8cab01a101d64370ad2664da29689627`](https://www.nowcoder.com/tutorial/10067/8cab01a101d64370ad2664da29689627)

# 三、归一化

归一化部分主要包括以下几个方面：

**1\. 什么是归一化
2\. 为什么需要归一化
3\. 归一化方法**

### 1、什么是归一化

假设你是蓝莓西瓜冰的爱好者，特别喜欢吃这款雪糕，于是，你想亲自动手做一份。你去超市买了一个 10 斤的西瓜和 250 克的蓝莓(国内蓝莓价格贵)。
这时候，你有两种方式来决定西瓜和蓝莓的用量：
第一种是按照个数，比如一个西瓜放一个蓝莓(半个西瓜放半个蓝莓)；
第二种是按比例，比如 200 克西瓜放 220 克蓝莓。

通常来说，大家不会选第一种，因为这种方式会使得“蓝莓西瓜冰”绝大部分只有西瓜，蓝莓的味道完全埋没在西瓜里；而第二种的方式，把两者的用量按照比例的方式控制在一定的范围内，所以两者的量不会有很大差距，很好的保留了两者的味道。

归一化可以简单地类比为第二种方式，在机器学习中，归一化是非常重要的一个步骤，它通过归一化和标准化的方法，改变特征值的“尺度”，把特征值“缩放”到一定的范围(如 0 到 1 内)，让所有的特征得到平等地对待，使得数据的处理保持一致，从而改善机器学习算法的表现并得到较好的模型。

### 2、为什么需要归一化

回顾一下，我们在使用一些机器学习算法时，是将一列列的数值传递给算法。
对算法而言，它只认识数字，对数字表示的含义却不知晓，比如 10 斤和 100 元，它不清楚“斤”和“元”所表示的意思；相反，算法认为 100 远大于 10，所以 100 会更重要些，在处理时，100 也会占据主导地位(比如一个西瓜和一个蓝莓混合是，西瓜占主要)，而较小的数值会被忽视(比如损失函数可能会被较大的特征值所主导)，从而影响模型表现，产生较差的预测结果。为了消除特征值之间的影响，需要进行归一化处理，以解决特征指标值之间的可比性。
原始数据经过归一化处理后，各特征值处于同一数量级，以便进行综合对比评价。

第二个原因是归一化可以使得梯度下降算法更快地收敛。梯度下降的过程，可以理解为从山顶找到一条下山的路。归一化可以使得这条下山路既平稳距离又短；而没有经过归一化的，既陡峭距离又长。

下图中(x1、x2 表示特征值; w1、w2 代表我们要学习的参数;J(w)表示损失函数)，左侧部分是没有进行归一化，x1 远大于 x2，需要经过多次迭代才能收敛；而右侧部分，经过了归一化，x1、x2 的值都处于 0 至 1 范围之内，只需要两到三次迭代，就可以收敛。

![`uploadfiles.nowcoder.com/files/20210201/897353_1612162455072/0081Kckwly1gl77wgmtrvj312i0jyguc.jpg`](img/2f15a01dc06e5e33c04df23cee857d6a.png)

第三个原因是一些机器学习算法对特征值的尺度比较敏感，具体如下：

*   KNN：需要计算欧几里得距离。
*   K-Means：需要计算欧几里得距离。
*   PCA(主成分分析)：特征向量将偏向值较大的列。
*   逻辑回归、支持向量机、神经网络：使用梯度下降来拟合参数的话，会影响收敛速度。

### 3、 归一化方法

特征归一化或特征缩放主要有两种方式，第一种是归一化(Normalization)，它会把特征值缩放到 0,1 区间或-1,1 区间；第二种是标准化(standardization)，它会使得特征值满足均值为 0，方差为 1。

具体有三种实现方法：

1.  min-max 归一化
2.  z-score 归一化
3.  L2 范数归一化

**数据集**介绍：数据集使用的是 scikit learn 自带的 boston 房价数据集。链接为：[`scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html#sklearn.datasets.load_boston`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html#sklearn.datasets.load_boston)

在进行归一化之前，先来看一下这些特征值的分布情况。

```cpp
import pandas as pd
from sklearn.datasets import load_boston

boston = load_boston()
df = pd.DataFrame(boston.data,columns=boston.feature_names)

df.hist(figsize=(15,15))

df.describe()
```

![`uploadfiles.nowcoder.com/files/20210201/897353_1612162455085/0081Kckwly1gl7at1f6t7j30od0nyjv1.jpg`](img/0864172390d2fc1b212faec3be905974.png)

![`uploadfiles.nowcoder.com/files/20210201/897353_1612162455087/0081Kckwly1gl7aw8lqf2j31hw0e80vj.jpg`](img/c116440c8a2f9b072720342dc56d3cbc.png)

从直方图以及 describe 函数得到的统计值可以看出，每个特征的均值、最小值、最大值和标准差差别很大 。

(1) min-max 归一化

计算公式为![](img/f39ec7c65591d81426c9f3b24c67711d.png),其中 X 为特征值，![](img/c3a674e60b80ce0e1d68d22eda5ce9ac.png)是该特征的最小值，![](img/28ec6b74b935be3cb519b3a970e006c6.png)是该特征的最大值。

这个公式会将特征值缩放到 0，1 区间之内。

使用 scikit learn 内置的 MinMaxScaler 函数来实现 min-max 归一化。

```cpp
from sklearn.preprocessing import MinMaxScaler

# 实例化
min_max = MinMaxScaler()

# 使用 min-max 进行标准化
min_maxed_df = pd.DataFrame(min_max.fit_transform(df),columns=boston.feature_names)

# 得到描述性统计
min_maxed_df.describe()  
```

![`uploadfiles.nowcoder.com/files/20210201/897353_1612162455080/0081Kckwly1gl7eb85t2kj31h20dktb8.jpg`](img/467a61b42701df3c390d1f9aeb4d8064.png)

观察上面的统计值可以看到，各列的最小值为 0，最大值为 1.

![`uploadfiles.nowcoder.com/files/20210201/897353_1612162455089/0081Kckwly1gl7fzbunt1j30og0nywf6.jpg`](img/106a7c95d6fe5f1b8a37e5d9dee635a5.png)

结合直方图可以看到，X 轴更紧密了，数值都介于 0 跟 1 之间，y 轴没有变化。

**缺点**：由于 min-max 归一化方式用到了特征值的最大值和最小值，所以很容易会收受到离群值的影响，导致计算结果被“扭曲”。这时候可以使用四分位数来代替最大值和最小值。可以使用 sklean 的 RobustScaler(默认使用 Q1、Q3 这段范围的数据)来实现这种归一化，此外，这种方法在当有新数据加入时，可能导致 max 和 min 的变化，需要重新定义。

(2) z-score 归一化

z-score 来源于概率学里面的标准分数，z-score 使得特征值的均值为 0、标准差为 1。如果初始特征服从高斯分布,那么缩放后的特征也服从高斯分布。

计算公式为：![](img/a2bd67373e0df3cac0e6ca0f44b5652f.png)

x 表示特征值

![](img/8b31ba7d8f06bb142c69eeee3f320e98.png)表示该特征列的均值

![](img/20430667a68ae1e52cc9afcbcd09e9d8.png)表示该特征列的标准差

通过 scikit learn 内置的 StandardScaler 函数来实现 z-score 归一化

```cpp
from sklearn.preprocessing import StandardScaler

# 实例化
standard_scaler = StandardScaler()

# 使用 z-score 进行标准化
standard_df = pd.DataFrame(standard_scaler.fit_transform(df),columns=boston.feature_names)

# 得到描述性统计
standard_df.describe()
```

从下面的统计结果中，可以看到 mean 已经非常小了，接近为 0；标准差都为 1

![`uploadfiles.nowcoder.com/files/20210201/897353_1612162455081/0081Kckwly1gl7g9dwuojj31ii0gyn0y.jpg`](img/5bf421abf2b31076bcb5ef3e470f5f71.png)

直方图如下所示：

![`uploadfiles.nowcoder.com/files/20210201/897353_1612162455078/0081Kckwly1gl7gbzsxpsj30oh0nydgi.jpg`](img/1b842798233c58266e9e3e7d27d7a92a.png)

**缺点**：z-score 归一化方法默认数据符合正态分布；当数据并不符合正态分布时，这可能并不是最好的归一化方式，这时候需要其他方式，比如 min-max。

(3) L2 范数归一化

与前两个归一化方法不同的是，这个方法是针对行的，而非列的，它会保证每一行都有一个相同的范数。计算公式如下：

![](img/9323d3858d51af5517861290370ab174.png)

分子 x 表示每行的数据，即 x=(x1,x2,x3...,xn)

||x||就是 L2 范数，计算方式为![](img/3f80960b8715c67657b062330ee7ad74.png)。可以把每行数据当作行向量，而范数就是向量的长度。

在经过 L2 范数归一化之后，每行都有相同的范数。

使用 scikit learn 内置的 Normalizer 函数来实现这一过程，代码如下：

```cpp
from sklearn.preprocessing import Normalizer

# 实例化
normalize = Normalizer()

normalize_df = pd.DataFrame(normalize.fit_transform(df),columns=boston.feature_names)

# 计算各行的范数
(normalize_df**2).sum(axis=1)
"""
发现各行值全部为 1
0      1.0
1      1.0
2      1.0
3      1.0
4      1.0
      ... 
501    1.0
502    1.0
503    1.0
504    1.0
505    1.0
"""
```

直方图以及描述统计图如下：

![`uploadfiles.nowcoder.com/files/20210201/897353_1612162455079/0081Kckwly1gl7h891ujsj30of0ny0th.jpg`](img/ec0c5c451f00ac4e6097039e3688fdd9.png)

![`uploadfiles.nowcoder.com/files/20210201/897353_1612162455114/0081Kckwly1gl7h9dbze1j31ho0dkacw.jpg`](img/ea0b72cebabaf225dd2caa4904d14e5c.png)

* * *

# 四、特征稀疏处理

这一部分主要涵盖以下几个问题：

**1\. 什么是特征稀疏
2\. 为什么要进行特征稀疏
3\. 如何进行特征稀疏**

### 1、什么是特征稀疏

在介绍特征稀疏这个概念之前，先通过稀疏矩阵来理解一下“稀疏”一词。
在矩阵中，如果数值为 0 的元素数目远远多于非 0 元素的数目，并且非 0 元素分布无规律时，则称该矩阵为稀疏矩阵；与之相反，若非 0 元素数目占大多数时，则称该矩阵为稠密矩阵。

下面创建一个稀疏矩阵来直观的感受一下。

```cpp
import numpy as np

arr = np.array([[1, 0, 0, 1, 0, 0], [0, 0, 2, 0, 0, 1], [0, 0, 0, 2, 0, 0]])

"""
array([[1, 0, 0, 1, 0, 0],
       [0, 0, 2, 0, 0, 1],
       [0, 0, 0, 2, 0, 0]])
"""
# 计算矩阵的稀疏性
sparsity = 1.0 - np.count_nonzero(arr) / arr.size
# 0.7222222222222222
```

在了解了“稀疏”一词后，就可以比较容易的理解特征稀疏一词了，即将稠密的数据集(普通非稀疏数据)，转化为稀疏数据的过程。那么，为什么需要进行特征稀疏呢？

### 2、为什么要进行特征稀疏

要对特征进行稀疏表示，是因为特征稀疏可以带来一定的好处，具体如下：

**(1) 特征选择。**有的矩阵中的许多列与当前学习任务无关，在最小化损失函数的时候考虑这些无关的特征，虽然可以获得更小的训练误差，但在进行预测时，这些无关的特征反而会被考虑，从而影响了模型的表现，产生错误的预测。通过稀疏化后，特征选择可以去除这些列,学习器训练过程仅需在较小的矩阵上进行,学习的难度可能有所降低，准确性提高，模型的可解释性也会提高。特征稀疏实质上是对于庞大数据集的一种降维表示，或者说是信息的压缩，用尽可能少的资源表示尽可能多的信息，可以展示出隐藏在样本背后最纯粹质朴的特征。

**(2) 可解释性。**假设我们收集到的特征是 100 种，探究这 100 个特征与目标变量的关系是非常困难的，模型的参数至少得有 100 个。假设我们可以通过稀疏表示，将这 100 多个特征减少到几个，这几个特征上面提供的信息是巨大的，决策性的。那么我们只需要聚焦在这个几个特征上就好了，可以方便我们的分析。

**(3 计算和存储开销会减少。**稀疏矩阵己有很多高效的存储方法，比如对稀疏矩阵进行压缩，从而减轻存储负担，在进行运算的时候，速度也会加快。

### 3、如何进行特征稀疏

将稠密的特征转化为稀疏表示的这个过程称为字典学习，又称稀疏编码。
这个两个叫法有些差别，字典学习偏重于习得字典的过程；稀疏表示侧重于对样本进行稀疏表达的过程。这属于一种无监督学习方法，寻求一组超完备的基(或者可以简单理解为基向量)，使得我们能将特征 X 表示为这些基向量的线性组合。

字典学习的代价函数定义如下：

![`uploadfiles.nowcoder.com/files/20210201/897353_1612162455059/0081Kckwly1glc5fxf0pej30ra046t99.jpg`](img/730de0a1681bb99e08d937629c6a5392.png)

参数介绍：

*   m：输入向量的数目

*   x：输入向量

*   ϕ：基向量

*   aiϕi：向量 x 的稀疏表示

*   【第 1 项】重构项（reconstruction term），迫使稀疏编码算法为输入向量提供一个高拟合度的线性表达式

*   【第 2 项】稀疏惩罚项（sparsity penalty term），使输入向量的表达式变得“稀疏”，也就是系数向量 a 变得稀疏

*   λ：控制这两项式子的相对重要性

    首先，需要对上面的代价函数进行迭代，求解最小损失时的 a 和ϕ。每次迭代分两步：第一个是固定字典ϕ，逐个使用训练样本 x 来优化系数 a；第二个是固定系数向量 a，对字典ϕ进行优化。不断迭代，直至收敛，这就可以得到一组能够良好表示训练样本 x 的超完备基，也就是字典。

    接下来的步骤是稀疏表示。在我们获得字典后，对于新的数据样本，我们必须再次执行优化过程来得到所需的稀疏向量 a，这个稀疏向量就是这个输入向量 x 的一个稀疏表达了。

    常用的一个字典学习算法为 KSVD(其他算法还有 Lasso、OMP)，它广泛运用在图像处理、声音处理、生物学和文档分析，它通过奇异值分解的方式来实现字典和稀疏矩阵的更新，你可以对其自行探索。关于这部分的内容，你可以阅读周志华的《机器学习》的第十一章第五章“稀疏表示与字典学习”进行进一步的学习。

    **L1 正则化补充**

    在机器学习中，我们会经常接触到 L1 正则化(或 lasso 正则化)和 L2(Ridge 正则化)，它们跟在损失函数的后面，作为惩罚项的存在，对模型参数进行限制，从而避免过拟合，提高模型的泛化能力。对于线性回归模型，使用 L1 正则化的模型建叫做 Lasso 回归，使用 L2 正则化的模型叫做 Ridge 回归。L1 正则化除了可以防止过拟合之外，还可以使一些模型参数降为 0，产生稀疏矩阵，并使得这些参数对应的特征没有意义，选择真正有用的特征，从而用于特征选择。

    下面来看一下带 L1 正则化的损失函数，公式如下![](img/4232c43f5a607c3369dc40f30ca0d82f.png)，其中，![](img/fcf920c2f9dcd733125737a4f32260f2.png)表示原始的损失函数，![](img/2d4c849fbdde7f37e77bf8dfa2194b7a.png)表示正则化系数，由用户指定，![](img/1a1c71a6f136951104691f6a0d44865f.png)表示各个模型参数绝对值的和(这就是 L1 正则化)。我们都知道，机器学习的任务是通过一些方法，比如梯度下降，找到损失函数值最小时的模型参数。正则化是在这个基础上加了一个约束，即在![](img/e9d3d82c13c1a630f9c0929320bd3cd2.png)约束下，求出![](img/fcf920c2f9dcd733125737a4f32260f2.png)取最小值时的解。在了解了这些内容之后，简单介绍一下为什么 L1 正则化容易产生稀疏矩阵。

    假设模型只有两个参数![](img/34b74054d0fc3eac3efaabaab7be6c46.png)和![](img/8ab7a8083976a2baf9c9d62e60767367.png)，则令![](img/6a475dde2c2305b9e0c6946424e47ad4.png)，以此绘制![](img/fcf920c2f9dcd733125737a4f32260f2.png)和![](img/dffec75ad87de4510d2d63c7d998327d.png)的等值线图，其中椭圆代表![](img/fcf920c2f9dcd733125737a4f32260f2.png)等值线，方形代表![](img/71b41a4d4044eb8500e0f211568c7927.png)的等值线，如下图所示：

![`uploadfiles.nowcoder.com/files/20210201/897353_1612162455063/008eGmZEly1gmgcc1np2lj30hg0gygps.jpg`](img/ef7194aad8cea6f7cc5c138b638ceef9.png)

因为最优解要在损失函数![](img/fcf920c2f9dcd733125737a4f32260f2.png)和![](img/c86d0d27c29f74439bb9b118064bee87.png)正则项之间折中，即出现在两等值线相交(首次相交)的地方。这个交点的值是![](img/54f765f63c41827ea42d87dea1465423.png)。![](img/c86d0d27c29f74439bb9b118064bee87.png)与![](img/fcf920c2f9dcd733125737a4f32260f2.png)的交点常出现在坐标轴上，因此会有很多 w 值为 0，这也是 L1 容易产生稀疏矩阵的原因。与 L1 相比，L2 则不容易获得“稀疏”解，这部分在此不进行介绍，请自行探索，可参考周志华《机器学习》第十一章第四小节“嵌入式选择与 L1 正则化”。

* * *

### 五、相关面试题

**1\. 离散化有哪些方法？**
无监督离散化：等距、等频、K-means；有监督离散化：基于信息熵的离散化；基于卡方的离散化

**2\. 归一化有哪些方法？**
min-max 归一化，z-score 归一化，L2 范数归一化

**3\. 标准化和归一化异同?**
都是线性变换，将原有数据进行了放缩；归一化一般放缩到[0,1]，标准化则转为服从标准正态分布

**4\. L1、L2 正则化有什么异同?**
都可以防止过拟合；两者的公式不一样；L1 可产生稀疏矩阵，用于特征选择，而 L2 做不到

**5\. 为什么需要进行归一化?**
提高梯度下降法求解速度；一些机器学习算法对特征值的尺度比较敏感，如 KNN，Kmeans

**6\. 哪些机器学习算法不需要做归一化处理**
概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、RF。而像 Adaboost、GBDT、XGBoost、SVM、LR、KNN、KMeans 之类的最优化问题就需要归一化

* * *

### 六、参考资料

*   《机器学习》——周志华
*   《数据挖掘导论》
*   《[稀疏编码](https://blog.csdn.net/LK274857347/article/details/76864828)》
*   《利用 Python 进行数据分析》
*   [K-SVD](https://en.wikipedia.org/wiki/K-SVD)