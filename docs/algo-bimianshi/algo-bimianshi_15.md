# 第五章 第 1 节 特征离散精讲

> 原文：[`www.nowcoder.com/tutorial/10067/d096d9435ab545a682092cc1ab41244c`](https://www.nowcoder.com/tutorial/10067/d096d9435ab545a682092cc1ab41244c)

# 一、写在前面

在之前的文章中，我们对数据清洗的内容做了一些介绍，数据清洗的目的在于处理“脏”数据，提升数据质量，从而改善模型的表现。如果将机器学习或数据科学比作做饭的话，数据清洗则相当于洗菜、择菜，洗去脏的东西，择掉坏的叶子；而这篇文章的特征离散、归一化部分则相当于一个切菜的过程，把长短不齐的蔬菜切成长短一致的段，从而便于烹饪，比如特征离散的等距法会将连续数据切分成长度相同的区间，而归一化可以将大小不同的数据映射到 0,1 之间，这些都是一定程度上的“规整”。特征稀疏是特征选择的一部分内容，如同做不同的菜要选不同部位的肉一样，特征选择的目的是在于筛选出对模型真正有用的特征。

* * *

# 二、特征离散

特征离散部分主要包括以下几个内容：

**1\. 什么是特征离散化
2\. 为什么需要特征离散化
3\. 如何进行特征离散化**

### 1、什么是特征离散化

在介绍离散化这个概念之前，我们先来复习连续型变量和离散型变量这两个概念。

连续型变量是指变量的取值是某个区间内的一个点，其数值是连续不断的,相邻两个数值可作无限分割。举个例子，假设你的体重是 60kg，这其实不是一个准确的数字，只是一个近似值，精确到了“kg”这一单位。它真实值可能是 60kg+0.2g+1mg...就是它的单位可以无限的小下去，从而得到的值也是连续不断的。另一个例子就是，你跟朋友约了七点钟见面，这里的“七点钟”可以继续细化为分、秒、毫秒、微妙...连续不断。所以，连续型变量可能的取值是无穷无尽的。

离散型变量与连续型变量正好相对，它的取值是可以数得完的，可以一一列举出来。比如，这一枚骰子得到的结果只能有 1、2、3、4、5、6 这几种可能；一门考试只有通过和失败这两种结果。

连续型变量本质上是数值，它是衡量某项事物的数量；而离散型变量的本质是类别，描述某项事物的性质。

在重温了连续和离散的概念之后，给出特征离散化的定义，即将连续的数据根据一些方法划分成一个个小区间，然后将这一个区间内的所有值映射到某个对应的离散值。离散化实际上是将连续型数据转换为离散型数据。

在明确了特征离散化的概念之后，来看一下为什么要进行离散化。

### 2、为什么需要特征离散化

接下来，看一下需要特征离散化的几个原因：

*   离散化的特征相对于连续型特征更易理解。以身体质量指数 BMI(目前国际上常用的衡量人体胖瘦程度以及是否健康的一个标准)为例, 假设你的 BMI 为 22，你可能对这个数值没有概念，但如果告诉你 18.5 ~ 23.9 这个范围内属于正常，你就能知道自己是正常的了。这个时候数字所在的范围是重要的，而数字具体是多少可能就没有那么重要了，毕竟 20、22、23 所传达的信息是相同的。所以离散化可以让我们更容易理解这些特征。
*   算法需要。一些算法，比如决策树、随机森林、朴素贝叶斯等，是基于离散型数据开展的，所以要使用这些算法必须将数据转换为离散型的。比如一个特征是连续的，它的取值非常多，可以理解为取值无限，在构造分类器时，假设选定了这个特征作为分支变量，这样一下子就把数据划分的很细，导致划分过早结束，分类器的表现就不是很好，很不可信。因此在分类之前或分类过程中应该对其离散化处理，有效的离散化能减小算法的时间和空间开销，提高系统对样本的分类能力。
*   更好地把握特征，这方面也是针对算法本身说的。由于连续型变量是连续不断的、变化无穷，可以理解为它的自由度是无限的，因此与目标变量的相关性可能比较弱，或者是复杂的非线性关系。在离散化之后，连续型数据转换为分组或分类，与目标变量对应的分组就可以被较容易解读，比如 100 个人可能有几十个不同的年龄，这些不同且繁多的特征值与目标变量的关系可能很弱并且不好观察，但经过离散化后，只有几个年龄分组，这样与目标变量的关系就比较容易观察了；此外，特征离散化以后，还起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。
*   对数据起到了降噪的作用。把连续型数据划分成一个个区间，会减少异常数据的影响。比如一个特征是年龄大于三十岁是 1，否则 0。如果特征没有离散化，一个异常数据“年龄 300 岁”会给模型造成很大的干扰。异常值实际上就是数据中的噪声，一定程度上容易导致过拟合，噪声的存在破坏了数据本身的正常分布，模型为了拟合异常值常常会给模型的训练结果带来偏差。
*   离散化之后得到的稀疏矩阵，运算速度更快，计算结果易于存储。

下面来看一下特征离散化的一些方法。

### 3、如何进行特征离散化

离散化包含两个步骤，第一步是决定需要多少个离散值，第二步是如何将连续型值映射到这些离散值。第一步需要将连续型值进行排序，通过指定 n-1 个分割点，把它们分成 n 个区间；第二步就是将一个区间中的所有值映射到相同的离散值，每一个区间内的原始连续值无差别的看成同一个新的离散值。因此，离散化的问题就是决定选择多少个分割点和确定分割点位置的问题。

离散化方法主要分为两类，第一类是无监督，第二类是有监督。两类方法的差别在于是否使用了类别信息。

先来看一下**无监督离散化**方法，主要有以下四种：

(1) 等距法：将连续型数据均分成 N 个区间，每个区间拥有相同的宽度。例如,我们可以按 10 年为一段来将人们划分到多个年龄范围中:0~~9 岁的在区间 1 中、10~~19 岁的在区间 2 中。宽度的计算方法为![](img/736950f500ad00ce2fd3db9994b43e1c.png)，区间边界值为 最小值+width,最小值+2*width,….最小值+(N−1)*width 。这里只考虑边界，每个等份里面的实例数量可能不等。比如属性值在[0，60]之间，最小值为 0，最大值为 60，我们要将其分为 3 等分，则区间被划分为[0,20] 、[21,40] 、[41，60]，每个属性值对应属于它的那个区间。这种方式适用于特征分布比较均匀的情况.

可以使用 scikit-learn 的 KBinsDiscretizer 来做离散化，KBinsDiscretizer 的实例化有几个参数需要指定，n_bins 指定区间的个数，encode 指定编码方式，strategy 指定离散方法，代码如下：

```cpp
from sklearn.preprocessing import KBinsDiscretizer

arr = [[-2, 1, -4,   -1],
     [-1, 2, -3, -0.5],
     [ 0, 3, -2,  0.5],
     [ 1, 4, -1,    2]]
# 实例化，指定划分成 3 个区间，ordinal 用数字表示每个区间，uniform 代表等距
discretizer = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')

# 进行离散化
discretizer.fit_transform(X)
"""
从结果可以看出，分成了 0，1，2 三个区间，以第一列为例，-2 映射到 0 区间，-1 映射到 1 区间，0 和 1 映射到 2 区间
array([[0., 0., 0., 0.],
       [1., 1., 1., 0.],
       [2., 2., 2., 1.],
       [2., 2., 2., 2.]])
"""
```

等距法的特点：固定宽度分箱非常容易计算,但如果计数值中有比较大的缺口,就会产生很多没有任何数据的空箱子，这时候可以通过等频法来解决。此外，等距法还有可能受离群点的影响而性能不佳，同样可以使用等频法来规避这一问题。

(2) 等频法：将连续型数据划分成 N 个区间，每个区间包含大致相等的实例数量。比如说中位数(即二分位数)可以将数据划分为两半,其中一半数据点比中位数小,另一半数据点比中位数大；四分位数将数据四等分；十分位数将数据十等分 ,每个区间应该包含大约 10%的实例。

使用 scikit-learn 的 KBinsDiscretizer 进行处理，代码如下：

```cpp
# 生成一个 4*10 的随机数组
arr = np.random.random((4,10))
# strategy 指定为 quantile，n_bins 指定划分成四等分
discretizer = KBinsDiscretizer(n_bins=4, encode='ordinal', strategy='quantile')

arr_d =discretizer.fit_transform(arr)
"""
从结果可以看出，划分成了 0、1、2、3 四份，每一份的个数都相同
array([[3., 3., 3., 3., 3., 0., 0., 3., 1., 0.],
       [2., 1., 0., 1., 2., 2., 3., 1., 3., 2.],
       [0., 2., 1., 2., 1., 1., 1., 0., 2., 1.],
       [1., 0., 2., 0., 0., 3., 2., 2., 0., 3.]])
"""
```

也可以使用 pandas 的 qcut 方法，代码如下

```cpp
import numpy as np

data = np.random.randn(10)
"""
array([ 1.05339511,  0.81779947, -0.87914687,  1.36679486, -0.90658604,
        0.43242882, -0.77838649, -0.47657106, -0.20573229, -0.21166562])
"""

# 划成两份
categories = pd.qcut(data,2)

"""
从结果可以看出，分成了(-0.908, -0.209]和(-0.209, 1.367]这两个区间
[(-0.209, 1.367], (-0.209, 1.367], (-0.908, -0.209], (-0.209, 1.367], (-0.908, -0.209], (-0.209, 1.367], (-0.908, -0.209], (-0.908, -0.209], (-0.209, 1.367], (-0.908, -0.209]]
Categories (2, interval[float64]): [(-0.908, -0.209] < (-0.209, 1.367]]
"""

# 验证各个区间内元素的个数是否相同
pd.value_counts(categories)
"""
(-0.209, 1.367]     5
(-0.908, -0.209]    5
dtype: int64
"""

# 自定义分位数
categories = pd.qcut(data,[0,0.2,0.7,0.9,1])
"""
(-0.799, 0.548]     5
(0.548, 1.085]      2
(-0.908, -0.799]    2
(1.085, 1.367]      1
dtype: int64
"""
```

等频法的特点：可以根据数据的分布特点,进行自适应的区间划分；等频虽然可以保证每个箱子里的样本数量大致相等，但是有可能出现不同箱子之间跨度太大的情况，比如数据分布极端，两边数据多，中间数据很少，可能最终分箱的结果是一部分箱子的区间长度为 10，另一部分箱子的区间长度为 1000。

(3) K-means 离散化：在连续值上运用 K-means 聚类算法，每一个簇可以看作为一个区间。

```cpp
K-means 离散化同样可以使用 scikit-learn 的 KBinsDiscretizer 来实现，只是在实例化时，strategy 参数值不一样，代码具体如下：
# 指定 strategy 为 kmeans
discretizer = KBinsDiscretizer(n_bins=4, encode='ordinal', strategy='kmeans')
```

K-means 特点：与前两种方式不同的是，使用机器学习模型进行划分；不均匀样本和异常样本的存在会导致聚类中心的向极端值偏移从而影响划分结果。

(4) 在有些场景，我们有时候需要自定义场景，比如年龄，我们一般把 0 到 10 岁这个年龄段称为“儿童”，把 10 到 25 岁定义为青少年。自定义区间的示例代码如下：

```cpp
# 将这些数据划分为“18 到 25”、“26 到 35”、“35 到 60”以及“60 以上这四个区间
ages = [20, 22, 25, 27, 21, 23, 37, 31, 61, 45, 41, 32]

bins = [18, 25, 35, 60, 100]

# 使用 pandas 的 cut 方法进行划分
categories = pd.cut(ages, bins)

"""
下面结果展示了每个年龄对应的所在区间
[(18, 25], (18, 25], (18, 25], (25, 35], (18, 25], ..., (25, 35], (60, 100], (35, 60], (35, 60], (25, 35]]
Length: 12
Categories (4, interval[int64]): [(18, 25] < (25, 35] < (35, 60] < (60, 100]]
"""

# 查看每个区间的标记值
categories.codes
"""
array([0, 0, 0, 1, 0, 0, 2, 1, 3, 2, 2, 1], dtype=int8)
"""
# 统计每个区间内的数量
pd.value_counts(categories)
"""
(18, 25]     5
(35, 60]     3
(25, 35]     3
(60, 100]    1
dtype: int64
"""
# 指定每个区间的名称
group_names = ['Youth', 'YoungAdult', 'MiddleAged', 'Senior']
categories = pd.cut(ages, bins,labels=group_names)

"""
[Youth, Youth, Youth, YoungAdult, Youth, ..., YoungAdult, Senior, MiddleAged, MiddleAged, YoungAdult]
Length: 12
Categories (4, object): [Youth < YoungAdult < MiddleAged < Senior]
"""

pd.value_counts(categories)
"""
Youth         5
MiddleAged    3
YoungAdult    3
Senior        1
dtype: int64
"""
```

总结这几种无监督的离散化方法：不需要引入分类信息(目标值)，思路简单，并且容易理解；但对数据要求较高，对不均匀的数据不适用，并且对异常值很敏感。

接下来看一下**有监督离散化**

尽管无监督离散化的方法可以对特征进行离散，并且取得的效果要比不离散的好(比如拟合非线性、数据集中噪声较多、使用逻辑回归模型)，但实际上仍然有进步的空间，那就是引入类别信息，即有监督离散化。这种类型的离散化考虑了数据的类别信息，利用真实的数据类别信息对连续型数据进行离散化处理，并以这样的方式进行划分，从而提供最大类别信息。

本章会介绍两种有监督离散化的方法，即基于信息熵的离散化和基于卡方的离散化。

**(1) 基于信息熵的离散化**

先来看一下“熵”的概念。熵是不确定性的一种度量，熵越大，不确定性越大。以投掷硬币为例，它只有两种可能性，要么正面向上，要么反面向上。假设我们给它施以魔法，只让它正面向上，那么它就只有一种可能性了，熵达到了最小值，要小于有两种可能性时候的熵。与此相比，投掷一枚骰子的熵就要更大的多，因为它有六种可能性(每种可能发生的概率都相同)。

与熵相对的另一个概念叫作信息，它能够消除不确定性。信息与熵的意义相反，获取信息意味着消除不确定性。

熵的计算公式为![](img/f4035f8a88beb35caccdee84141013bf.png)

上式中，i=1 一直到 n，表示分类个数，比如掷硬币只有两种结果；![](img/243e6e62931625ba4c62da08d09c1e91.png)表示每种结果的概率值，![](img/2df2bf9fafa2477e7079860089628a3c.png)，其中，![](img/4fecd08c676a369420fe860b7ce28c74.png)表示划分的第 i 个区间中值的个数；![](img/ccf47c4ce6561de756dcdaa17bf43242.png)是区间 i 中类 j 的值的个数。

以投掷硬币为例，熵的计算公式为![](img/649e014a49804743ac510e46718a1587.png)；假设只有一种可能，则熵为![](img/069e092512d69f707f482396063c7252.png)。由此可见，最确定的时候，熵最小，为零。

在划分区间之后，该划分的总熵 e 是每个区间熵的加权平均，公式为![](img/cb1b3002f3374c5a52e816e32c2ae53d.png)，其中 n 表示区间总个数；w 表示权重，即区间内值的占比；e 表示每个区间的熵。

接下来介绍一下信息增益，它用来衡量熵减少的程度，熵从 1 到 0，则信息增益为 1。

离散化过程如下：

*   首先将属性的取值值域按照值的大小排序。

*   把每个值看作是可能的分割点，依次把区间分成两部分计算它们的熵值，取熵值最小的作为第一次划分点。

*   然后选取一个区间，通常选择熵值最大的区间重复上述过程。

*   当区间个数达到指定的个数或者满足终止条件(比如信息增益不再增加)后停止划分。

    **(2) 基于卡方的离散化**

    上面介绍了基于熵的离散化，这种离散化是一种自顶向下的划分策略，即先把数据集看做成一个整体的区间，然后将将大区间分成小区间，重复此过程，直至完成离散化。基于卡方的离散化过程正好与前者相反，它是一种自底向上的策略，它将数据集内所有的数据点都看作是一个独立的区间，然后逐一递归进行合并，使用卡方检验来决定是否合并，直至完成离散化。

    ChiMerge 方法是最常用的基于卡方的离散化方法，它的思路是将具有最小卡方值的相邻区间合并在一起,直到满足确定的停止准则，流程主要包括对数据进行排序，计算相邻区间的卡方值，合并卡方值最小的两项，重复第二步和第三步，直至满足停止条件(比如达到指定的分箱个数或最小卡方值大于卡方阈值)。

    接下来重点介绍卡方的计算。

    卡方的一般计算公式为：![](img/94a76d08d696eabc54aec43e5d3810fc.png)，其中 Ai 为观察频数，Ei 为理论频数(或期望频数)，k 为单元格数量。下面结合四格表理解一下这个公式。

    下图是一个四格表，在卡方检验中会被用到，图中的 B1、B2 表示两个分类，A1、A2 表示两个特征，而 a、b、c、d 表示属于 B1、B2 的频数，即公式中的 Ai(观察频数)。

![`uploadfiles.nowcoder.com/files/20210201/897353_1612162455056/008eGmZEly1gmf5w36vraj30m40dcglo.jpg`](img/4ca6ed6ad18dde1a53bbb3dd57bc1d6f.png)

观察频数是直接可以得到的，而理论频数则需要自己计算。先把 A1、A2 当作一个特征来看，则属于 B1 的概率为![](img/24f01a00f09d9c22cb3be3987dd16880.png), 属于 B2 的概率为![](img/22f1dc91d97ee80108027e9da783f2fc.png)。则 a 对应的理论频数为![](img/943bf58e0d049f58f40d2034e636a0aa.png)，b 对应的理论频数为![](img/27ab8cc353aa53ef392eaf1f2bcf2129.png)，c 对应的理论频数为![](img/9a696ff816f2e28318d83f110698b02e.png)，d 对应的理论频数为![](img/c8204483b195a01d62ba6bc187dc2eae.png)，有了理论频数后，就可以带入上面的计算公式来计算卡方值了。卡方值的计算公式的意义是衡量观测值与理论值的距离，如果卡方值小，说明观测值和理论值差距很小，即分布是类似的，可以合并。

**使用离散化后的特征**

在之前的代码中，参数 encode=’ordinal’ 表示使用数字，比如 1、2、3 表示每个分类。如果这种数字形式满足的你需要的话，可以直接使用这些数字；如果不是数字的话，可以通过转码(encoding)的方式，将其转为数字，并将数字与分类关联起来，用数字来代表分类。

* * *